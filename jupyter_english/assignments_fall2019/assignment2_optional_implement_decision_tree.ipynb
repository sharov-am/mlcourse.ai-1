{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "Author: [Yury Kashnitsky](https://yorko.github.io) (@yorko). Edited by Anna Tarelina (@feuerengel). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment #2. Optional part\n",
    "## <center> Implementation of the decision tree algorithm\n",
    "    \n",
    "#  <center>  <font color = 'red'> Warning! </font>This is a very useful but ungraded assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_classification, make_regression, load_digits, load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix `random_state` (a.k.a. random seed) beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Implement the class `DecisionTree`**\n",
    "**Specification:**\n",
    "- the class is inherited from `sklearn.BaseEstimator`;\n",
    "- class constructor has the following parameters: \n",
    "    `max_depth` - maximum depth of the tree (`numpy.inf` by default); \n",
    "    `min_samples_split` - the minimum number of instances in a node for a splitting to be done (2 by default); \n",
    "    `criterion` - split criterion ('gini' or 'entropy' for classification, 'variance' or 'mad_median' for regression; 'gini' by default);\n",
    "    \n",
    "    A functional to be maximized to find an optimal partition at a given node has the form\n",
    "    $$Q(X, j, t) = F(X) - \\dfrac{|X_l|}{|X|} F(X_l) - \\dfrac{|X_r|}{|X|} F(X_r),$$\n",
    "    where $X$ are samples at a given node, $X_l$ and $X_r$ are partitions of samples $X$ into two parts \n",
    "    with the following condition $[x_j < t]$, and $F(X)$ is a partition criterion.\n",
    "    \n",
    "    For classification: let $p_i$ be the fraction of the instances of the $i$-th class in the dataset $X$.\n",
    "    \n",
    "    'gini': Gini impurity $F(X) = 1 -\\sum_{i = 1}^K p_i^2$.\n",
    "    \n",
    "    'entropy': Entropy $F(X) = -\\sum_{i = 1}^K p_i \\log_2(p_i)$.\n",
    "    \n",
    "    For regression: $y_j = y(x_j)$ - is a target for an instance $x_j$, $y = (y_1, \\dots, y_{|X|})$ - is a target vector.\n",
    "    \n",
    "    'variance': Variance (mean quadratic deviation from average) $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}(y_j - \\dfrac{1}{|X|}\\sum_{x_i \\in X}y_i)^2$\n",
    "    \n",
    "    'mad_median': Mean deviation from the median $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}|y_j - \\mathrm{med}(y)|$\n",
    "    \n",
    "- the class has several methods: `fit`, `predict` and `predict_proba`;\n",
    "- the`fit` method takes the matrix of instances `X` and a target vector `y` (`numpy.ndarray` objects) and returns an instance of the class `DecisionTree` representing the decision tree trained on the dataset `(X, y)` according to parameters set in the constructor; \n",
    "- the `predict_proba` method takes the matrix of instances `X` and returns the matrix `P` of a size `X.shape[0] x K`, where `K` is the number of classes and $p_{ij}$ is the probability of an instance in $i$-th row of `X` to belong to class $j \\in \\{1, \\dots, K\\}$.\n",
    "- the `predict` method takes the matrix of instances `X` and returns a prediction vector; in case of classification, prediction for an instance $x_i$ falling into leaf $L$ will be the class, mostly represented among instances in $L$. In case of regression, it'll be the mean value of targets for all instances in leaf $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):    \n",
    "    e = 0\n",
    "    for v in y:\n",
    "        e += -v *np.log2(v)\n",
    "\n",
    "def gini(y):\n",
    "    sum = 0\n",
    "    for i in y:\n",
    "        sum += i**2\n",
    "    return 1 - sum    \n",
    "\n",
    "def variance(y):\n",
    "     return np.var(y) \n",
    "\n",
    "def mad_median(y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Node` class implements a node in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \n",
    "    def __init__(self, feature_idx=0, threshold=0, labels=None, left=None, right=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.labels = labels\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the function for calculating a prediction in a leaf. For regression, let's take the mean for all values in a leaf, for classification - the most popular class in leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, max_depth=np.inf, min_samples_split=2, \n",
    "                 criterion='gini', debug=False):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        if criterion == 'gini':\n",
    "            self.crit = gini\n",
    "        else:\n",
    "            self.crit = entropy\n",
    "        \n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.root_node = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        print(X)\n",
    "        print(y)\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        max_q = float(\"-inf\")\n",
    "        best_split_value = None\n",
    "        best_feature = None\n",
    "        y_l = None\n",
    "        y_r = None\n",
    "        for f in range(X.shape[0]):\n",
    "            x_row = X[f]\n",
    "            splits = self._find_splits(x_row)\n",
    "            for spl in splits: \n",
    "        \n",
    "              q,yl,yr = self._regression_var_criterion(X,f, y ,spl)\n",
    "              if q > max_q:\n",
    "                max_q = q\n",
    "                best_split_value = spl\n",
    "                best_feature = f\n",
    "                X_left = X[X[f] < best_split_value]\n",
    "                X_right = X[X[f] >= best_split_value]\n",
    "                y_l = yl\n",
    "                y_r = yr\n",
    "        \n",
    "        \n",
    "        \n",
    "        if max_depth == 0:\n",
    "            return Node(np.where(features == f),best_split_value,'leaf', None ,None)\n",
    "        \n",
    "        self.left_child =  self.fit(X_left,y_l) \n",
    "        self.right_child =  self.fit(X_right,y_r) \n",
    "        self.root_node = Node(np.where(features == f), best_split_value, str(f),y_l,y_r)       \n",
    "        \n",
    "        return self        \n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = []\n",
    "        for x in X:\n",
    "          \n",
    "          cur_node = self.root_node\n",
    "          leaf = cur_node\n",
    "          while cur_node is not None:   \n",
    "            \n",
    "            leaf = cur_node\n",
    "            if x[cur_node.feature_idx] < cur_node.threshold:\n",
    "              cur_node = cur_node.left_child\n",
    "            else:\n",
    "              cur_node = cur_node.right_child\n",
    "            \n",
    "            type = X.dtypes[leaf.feature_idx]\n",
    "            if type == 'object' :\n",
    "                predictions.append(most_frequent(np.append(leaf.left,leaf.right)))\n",
    "            else:\n",
    "                predictions.append(np.mean(np.append(leaf.left,leaf.right)))\n",
    "        return predictions  \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "\n",
    "        # Get unique values in a sorted order\n",
    "        x_unique = list(np.unique(X))\n",
    "        for i in range(1, len(x_unique)):\n",
    "            # Find a point between two values\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "    \n",
    "    \n",
    "    def most_frequent(List): \n",
    "        return max(set(List), key = List.count) \n",
    "    \n",
    "    def _regression_var_criterion(self, X, feature, y, t):\n",
    "          x_col_values = X[feature]\n",
    "          y_l = [y[i] for i, val in enumerate(x_col_values) if val < t]\n",
    "          y_r = [y[i] for i, val in enumerate(x_col_values) if val >= t]\n",
    "          q = np.var(y)  - len(y_l)/len(y) * self.crit(y_l) - len(y_r)/len(y) * self.crit(y_r)\n",
    "       \n",
    "          return q,y_l,y_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the implemented algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `digits` using the method `load_digits`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, and `random_state=17`. Try to train shallow decision trees and make sure that gini and entropy criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  3. ... 16.  2.  0.]\n",
      " [ 0.  0.  6. ...  0.  0.  0.]\n",
      " [ 0.  1.  7. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  6. 16. ... 11.  1.  0.]\n",
      " [ 0.  1.  8. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 16. 16. 16.]]\n",
      "[1 9 5 ... 3 7 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1437 but corresponding boolean dimension is 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9e21e2a1df10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-81a93e4cc67d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mbest_split_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mbest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mX_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_split_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mX_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_split_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0my_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1437 but corresponding boolean dimension is 64"
     ]
    }
   ],
   "source": [
    "X,y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use range(3, 11), for criterion use {'gini', 'entropy'}. Quality measure is `scoring`='accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'max_depth': list(range(3, 11)),'criterion': ['gini', 'entropy']}\n",
    "tree_grid = GridSearchCV(DecisionTree(debug=True), tree_params, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `accuracy` for criteria `gini` and `entropy` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv': 5,\n",
      " 'error_score': 'raise-deprecating',\n",
      " 'estimator': DecisionTree(criterion=None, debug=None, max_depth=inf, min_samples_split=2),\n",
      " 'iid': 'warn',\n",
      " 'n_jobs': None,\n",
      " 'param_grid': {'criterion': ['gini', 'entropy'],\n",
      "                'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]},\n",
      " 'pre_dispatch': '2*n_jobs',\n",
      " 'refit': True,\n",
      " 'return_train_score': False,\n",
      " 'scoring': 'accuracy',\n",
      " 'verbose': 0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dc3a6715e1cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m plt.plot(tree_params['max_depth'], \n\u001b[0;32m----> 5\u001b[0;31m          tree_grid.cv_results_['mean_test_score'])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max depth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean CV accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(vars(tree_grid))\n",
    "\n",
    "plt.plot(tree_params['max_depth'], \n",
    "         tree_grid.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Mean CV accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Choose all correct statements:**\n",
    "1. Optimal value of the `max_depth` parameter is on the interval [4, 9] for both criteria.\n",
    "2. Created plots have no intersection on the interval [3, 10]\n",
    "3. Created plots intersect each other only once on the interval [3, 10].\n",
    "4. The best quality for `max_depth` on the interval [3, 10] is reached using `gini` criterion .\n",
    "5. Accuracy is strictly increasing at least for one of the criteria, when `max_depth` is also increasing on the interval [3, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the optimal values for max_depth and criterion parameters?**\n",
    "1. max_depth = 7, criterion = 'gini';\n",
    "2. max_depth = 7, criterion = 'entropy';\n",
    "3. max_depth = 10, criterion = 'entropy';\n",
    "4. max_depth = 10, criterion = 'gini';\n",
    "5. max_depth = 9, criterion = 'entropy';\n",
    "6. max_depth = 9, criterion = 'gini';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train decision tree on `(X_train, y_train)` using the optimal values of `max_depth` and `criterion`. Compute class probabilities for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given matrix, compute the mean class probabilities for all instances in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is the maximum probability in a resulted vector?**\n",
    "1. 0.127\n",
    "2. 0.118\n",
    "3. 1.0\n",
    "4. 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `boston` using the method `load_boston`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, `random_state=17`. Try to train shallow regression decision trees and make sure that `variance` and `mad_median` criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use `range(2, 9)`, for `criterion` use {'variance', 'mad_median'}. Quality measure is `scoring`='neg_mean_squared_error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `neg_mean_squared_error` for criteria `variance` and `mad_median` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Choose all correct statements:**\n",
    "1. Created plots have no intersection on the interval [2, 8].\n",
    "2. Created plots intersect each other only once on the interval [2, 8].\n",
    "3. Optimal value of the `max_depth` for each of the criteria is on the border of the interval [2, 8].\n",
    "4. The best quality at `max_depth` on the interval [2, 8] is reached using `mad_median` criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What are the optimal values for `max_depth` and `criterion` parameters?**\n",
    "1. max_depth = 9, criterion = 'variance';\n",
    "2. max_depth = 5, criterion = 'mad_median';\n",
    "3. max_depth = 4, criterion = 'variance';\n",
    "4. max_depth = 2, criterion = 'mad_median';\n",
    "5. max_depth = 4, criterion = 'mad_median';\n",
    "6. max_depth = 5, criterion = 'variance'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "name": "lesson4_part2_Decision_trees.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
